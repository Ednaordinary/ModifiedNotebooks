{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LZDCXw5_bT4P"},"outputs":[],"source":["# Run this line in Colab to install the package if it is\n","# not already installed.\n","!pip install git+https://github.com/openai/glide-text2im\n","!apt install -y ffmpeg\n","!wget https://raw.githubusercontent.com/Ednaordinary/ModifiedNotebooks/main/greymask.png\n","!wget https://raw.githubusercontent.com/Ednaordinary/ModifiedNotebooks/main/greymask64.png"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"GdjlhyFvbT4T","executionInfo":{"status":"ok","timestamp":1640740651985,"user_tz":420,"elapsed":136,"user":{"displayName":"Tech Splosion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYUebn86msl3M9uXowwlLzsWi26IJONBMOEg1x=s64","userId":"02655816759197812093"}}},"outputs":[],"source":["from typing import Tuple\n","\n","from IPython.display import display\n","from IPython.display import HTML\n","from base64 import b64encode\n","from PIL import Image\n","import numpy as np\n","import torch as th\n","import torch.nn.functional as F\n","\n","from glide_text2im.download import load_checkpoint\n","from glide_text2im.model_creation import (\n","    create_model_and_diffusion,\n","    model_and_diffusion_defaults,\n","    model_and_diffusion_defaults_upsampler\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fBc0iqWebT4U","executionInfo":{"status":"ok","timestamp":1640737790453,"user_tz":420,"elapsed":248,"user":{"displayName":"Tech Splosion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYUebn86msl3M9uXowwlLzsWi26IJONBMOEg1x=s64","userId":"02655816759197812093"}}},"outputs":[],"source":["# This notebook supports both CPU and GPU.\n","# On CPU, generating one sample may take on the order of 20 minutes.\n","# On a GPU, it should be under a minute.\n","\n","has_cuda = th.cuda.is_available()\n","device = th.device('cpu' if not has_cuda else 'cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixAIkeyfbT4V"},"outputs":[],"source":["# Create base model.\n","options = model_and_diffusion_defaults()\n","options['inpaint'] = True\n","options['use_fp16'] = has_cuda\n","options['timestep_respacing'] = '10' # use 2 iteation for animation\n","model, diffusion = create_model_and_diffusion(**options)\n","model.eval()\n","if has_cuda:\n","    model.convert_to_fp16()\n","model.to(device)\n","model.load_state_dict(load_checkpoint('base-inpaint', device))\n","print('total base parameters', sum(x.numel() for x in model.parameters()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jh4uYwtYbT4W"},"outputs":[],"source":["# Create upsampler model.\n","options_up = model_and_diffusion_defaults_upsampler()\n","options_up['inpaint'] = True\n","options_up['use_fp16'] = has_cuda\n","options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n","model_up, diffusion_up = create_model_and_diffusion(**options_up)\n","model_up.eval()\n","if has_cuda:\n","    model_up.convert_to_fp16()\n","model_up.to(device)\n","model_up.load_state_dict(load_checkpoint('upsample-inpaint', device))\n","print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"TtvEPkpvbT4X","executionInfo":{"status":"ok","timestamp":1640738407507,"user_tz":420,"elapsed":10,"user":{"displayName":"Tech Splosion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYUebn86msl3M9uXowwlLzsWi26IJONBMOEg1x=s64","userId":"02655816759197812093"}}},"outputs":[],"source":["def show_images(batch: th.Tensor):\n","    \"\"\" Display a batch of images inline. \"\"\"\n","    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n","    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n","    display(Image.fromarray(reshaped.numpy()))\n","\n","def read_image(path: str, size: int = 256) -> Tuple[th.Tensor, th.Tensor]:\n","    pil_img = Image.open(path).convert('RGB')\n","    pil_img = pil_img.resize((size, size), resample=Image.BICUBIC)\n","    img = np.array(pil_img)\n","    return th.from_numpy(img)[None].permute(0, 3, 1, 2).float() / 127.5 - 1\n","\n","def save_images(batch: th.Tensor, prompt, num):\n","    \"\"\" save a batch of images inline. \"\"\"\n","    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n","    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n","    genimage = Image.fromarray(reshaped.numpy())\n","    filename = prompt.replace(\" \", \"_\")\n","    genimage = genimage.save(f\"./{filename}_{num}.jpg\")"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"SjpAUG6sbT4Y","executionInfo":{"status":"ok","timestamp":1640739823455,"user_tz":420,"elapsed":17,"user":{"displayName":"Tech Splosion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYUebn86msl3M9uXowwlLzsWi26IJONBMOEg1x=s64","userId":"02655816759197812093"}}},"outputs":[],"source":["# Sampling parameters\n","prompt = \"a corgi in a field\"\n","batch_size = 1\n","guidance_scale = 5.0\n","video_fps = 2\n","\n","# Tune this parameter to control the sharpness of 256x256 images.\n","# A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n","upsample_temp = 0.997"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KiRT_FnUbT4a"},"outputs":[],"source":["##############################\n","# Sample from the base model #\n","##############################\n","\n","# Create the text tokens to feed to the model.\n","tokens = model.tokenizer.encode(prompt)\n","tokens, mask = model.tokenizer.padded_tokens_and_mask(\n","    tokens, options['text_ctx']\n",")\n","\n","# Create the classifier-free guidance tokens (empty)\n","full_batch_size = batch_size * 2\n","uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n","    [], options['text_ctx']\n",")\n","\n","# Source image we are inpainting\n","source_image_256 = read_image('./greymask.png', size=256)\n","source_image_64 = read_image('./greymask64.png', size=64)\n","\n","# The mask should always be a boolean 64x64 mask, and then we\n","# can upsample it for the second stage.\n","source_mask_64 = th.ones_like(source_image_64)[:, :1]\n","source_mask_64[:, :, 20:] = 0\n","source_mask_256 = F.interpolate(source_mask_64, (256, 256), mode='nearest')\n","\n","num = 0\n","\n","# Pack the tokens together into model kwargs.\n","model_kwargs = dict(\n","    tokens=th.tensor(\n","        [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n","    ),\n","    mask=th.tensor(\n","        [mask] * batch_size + [uncond_mask] * batch_size,\n","        dtype=th.bool,\n","        device=device,\n","    ),\n","\n","    # Masked inpainting image\n","    inpaint_image=(source_image_64 * source_mask_64).repeat(full_batch_size, 1, 1, 1).to(device),\n","    inpaint_mask=source_mask_64.repeat(full_batch_size, 1, 1, 1).to(device),\n",")\n","\n","# initial iteration\n","\n","# Create an classifier-free guidance sampling function\n","def model_fn(x_t, ts, **kwargs):\n","  half = x_t[: len(x_t) // 2]\n","  combined = th.cat([half, half], dim=0)\n","  model_out = model(combined, ts, **kwargs)\n","  eps, rest = model_out[:, :3], model_out[:, 3:]\n","  cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n","  half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n","  eps = th.cat([half_eps, half_eps], dim=0)\n","  return th.cat([eps, rest], dim=1)\n","\n","def denoised_fn(x_start):\n","  # Force the model to have the exact right x_start predictions\n","  # for the part of the image which is known.\n","  return (\n","      x_start * (1 - model_kwargs['inpaint_mask'])\n","      + model_kwargs['inpaint_image'] * model_kwargs['inpaint_mask']\n","  )\n","\n","# Sample from the base model.\n","model.del_cache()\n","samples = diffusion.p_sample_loop(\n","    model_fn,\n","    (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n","    device=device,\n","    clip_denoised=True,\n","    progress=True,\n","    model_kwargs=model_kwargs,\n","    cond_fn=None,\n","    denoised_fn=denoised_fn,\n","    )[:batch_size]\n","model.del_cache()\n","\n","save_images(samples, prompt, num)\n","\n","for i in range(49):\n","\n","  # generated of last iteration\n","  filename = prompt.replace(\" \", \"_\")\n","  source_image_256 = read_image(f'./{filename}_{num}.jpg', size=256)\n","  source_image_64 = read_image(f'./{filename}_{num}.jpg', size=64)\n","  \n","  # The mask should always be a boolean 64x64 mask, and then we\n","  # can upsample it for the second stage.\n","  source_mask_64 = th.ones_like(source_image_64)[:, :1]\n","  source_mask_64[:, :, 20:] = 0\n","  source_mask_256 = F.interpolate(source_mask_64, (256, 256), mode='nearest')\n","\n","  model_kwargs = dict(\n","      tokens=th.tensor(\n","        [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n","    ),\n","    mask=th.tensor(\n","        [mask] * batch_size + [uncond_mask] * batch_size,\n","        dtype=th.bool,\n","        device=device,\n","    ),\n","\n","    # Masked inpainting image\n","    inpaint_image=(source_image_64 * source_mask_64).repeat(full_batch_size, 1, 1, 1).to(device),\n","    inpaint_mask=source_mask_64.repeat(full_batch_size, 1, 1, 1).to(device),\n","  )\n","\n","\n","  # Create an classifier-free guidance sampling function\n","  def model_fn(x_t, ts, **kwargs):\n","     half = x_t[: len(x_t) // 2]\n","     combined = th.cat([half, half], dim=0)\n","     model_out = model(combined, ts, **kwargs)\n","     eps, rest = model_out[:, :3], model_out[:, 3:]\n","     cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n","     half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n","     eps = th.cat([half_eps, half_eps], dim=0)\n","     return th.cat([eps, rest], dim=1)\n","  \n","  def denoised_fn(x_start):\n","    # Force the model to have the exact right x_start predictions\n","    # for the part of the image which is known.\n","    return (\n","        x_start * (1 - model_kwargs['inpaint_mask'])\n","        + model_kwargs['inpaint_image'] * model_kwargs['inpaint_mask']\n","    )\n","  \n","  # Sample from the base model.\n","  model.del_cache()\n","  samples = diffusion.p_sample_loop(\n","      model_fn,\n","      (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n","      device=device,\n","      clip_denoised=True,\n","      progress=False,\n","      model_kwargs=model_kwargs,\n","      cond_fn=None,\n","      denoised_fn=denoised_fn,\n","      )[:batch_size]\n","  model.del_cache()\n","\n","  num += 1\n","\n","  save_images(samples, prompt, num)\n","\n","# Show the output\n","show_images(samples)"]},{"cell_type":"code","source":["# make the video and display it\n","filename = prompt.replace(\" \", \"_\")\n","!ffmpeg -i ./{filename}_%d.jpg -r {video_fps} ./{filename}.mp4\n","mp4 = open(f'{filename}.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=640 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"metadata":{"id":"uLT7jc4skcdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# purge all the jpgs from the file tree (not necessary, just to clean things up)\n","!rm ./*.jpg"],"metadata":{"id":"IbKm1Mg4rPZ5","executionInfo":{"status":"ok","timestamp":1640740912708,"user_tz":420,"elapsed":279,"user":{"displayName":"Tech Splosion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYUebn86msl3M9uXowwlLzsWi26IJONBMOEg1x=s64","userId":"02655816759197812093"}}},"execution_count":30,"outputs":[]}],"metadata":{"interpreter":{"hash":"e7d6e62d90e7e85f9a0faa7f0b1d576302d7ae6108e9fe361594f8e1c8b05781"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU","colab":{"name":"inpaint-animation.ipynb","provenance":[{"file_id":"https://github.com/openai/glide-text2im/blob/main/notebooks/inpaint.ipynb","timestamp":1640736327025}]}},"nbformat":4,"nbformat_minor":0}